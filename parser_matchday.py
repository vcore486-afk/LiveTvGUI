import requests
from bs4 import BeautifulSoup
import os
import argparse

# =========================
# –ß—Ç–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –¥–æ–º–µ–Ω–∞
# =========================
def read_base_domain():
    config_path = os.path.join(os.path.expanduser("~"), ".livetv", "config.txt")
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            base_domain = f.read().strip()
            if not base_domain:
                raise ValueError("Base domain is empty in the config file.")
            return base_domain
    except FileNotFoundError:
        print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        raise
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {e}")
        raise


# =========================
# –ß—Ç–µ–Ω–∏–µ –ø—Ä–æ–∫—Å–∏
# =========================
def read_proxy():
    proxy_path = os.path.join(os.path.expanduser("~"), ".livetv", "proxy.txt")

    try:
        config = {}
        with open(proxy_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or "=" not in line:
                    continue
                key, value = line.split("=", 1)
                config[key.strip()] = value.strip()

        host = config.get("host")
        port = config.get("port")
        ptype = config.get("type", "http")
        user = config.get("user")
        password = config.get("password")

        if not host or not port:
            raise ValueError("host –∏–ª–∏ port –Ω–µ —É–∫–∞–∑–∞–Ω—ã –≤ proxy.txt")

        if user and password:
            proxy_url = f"{ptype}://{user}:{password}@{host}:{port}"
        else:
            proxy_url = f"{ptype}://{host}:{port}"

        return {
            "http": proxy_url,
            "https": proxy_url,
        }

    except FileNotFoundError:
        print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª {proxy_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        raise
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ proxy.txt: {e}")
        raise


# =========================
# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫
# =========================
BASE_DOMAIN = read_base_domain()
PROXIES = read_proxy()


# =========================
# –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
# =========================
def parse_page(page_number):
    URL = f"{BASE_DOMAIN}/allupcomingsports/{page_number}/"

    homePath = os.path.expanduser("~")
    output_dir = os.path.join(homePath, ".livetv")
    os.makedirs(output_dir, exist_ok=True)

    OUTPUT_FILE = os.path.join(output_dir, "matchday_events.txt")

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
    try:
        response = requests.get(
            URL,
            headers={"User-Agent": "Mozilla/5.0"},
            proxies=PROXIES,
            timeout=15
        )
        response.raise_for_status()
        html_content = response.text
    except requests.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return

    # 2. –ü–∞—Ä—Å–∏–º HTML
    soup = BeautifulSoup(html_content, "lxml")

    # 3. –ù–∞—Ö–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    header = soup.find("b", text="–ì–ª–∞–≤–Ω—ã–µ –º–∞—Ç—á–∏ –¥–Ω—è")
    if not header:
        print("‚ùå –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return

    container = header.find_parent("table")
    matches_found = 0

    # 4. –ó–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª
    with open(OUTPUT_FILE, "w", encoding="utf-8") as out:
        out.write(f"üî• –ì–ª–∞–≤–Ω—ã–µ –º–∞—Ç—á–∏ –¥–Ω—è (–ø–µ—Ä–≤—ã–µ 10) - –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number}:\n\n")

        for a in container.find_all_next("a", class_="live"):
            match = a.get_text(strip=True)
            raw_href = a["href"].lstrip("/")

            if raw_href.startswith("allupcomingsports/"):
                raw_href = raw_href[len("allupcomingsports/"):]

            link = f"{BASE_DOMAIN}/{raw_href}"

            evdesc = a.find_next("span", class_="evdesc")
            if not evdesc:
                continue

            lines = list(evdesc.stripped_strings)
            date_time = lines[0]
            league = lines[-1].strip("()")

            out.write(f"{league}\t{match}\n")
            out.write(f"{date_time}\n")
            out.write(f"({league})\n")
            out.write(f"{link}\n\n")

            matches_found += 1
            if matches_found == 10:
                break

    if matches_found == 0:
        print("‚ùå –ú–∞—Ç—á–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    else:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –º–∞—Ç—á–µ–π: {matches_found}")
        print(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {OUTPUT_FILE}")


# =========================
# –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
# =========================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ü–∞—Ä—Å–∏–Ω–≥ –º–∞—Ç—á–µ–π —Å —Å–∞–π—Ç–∞")
    parser.add_argument("page_number", type=int, help="–ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
    args = parser.parse_args()

    homePath = os.path.expanduser("~")
    output_dir = os.path.join(homePath, ".livetv")
    OUTPUT_FILE = os.path.join(output_dir, "matchday_events.txt")

    if os.path.exists(OUTPUT_FILE):
        os.remove(OUTPUT_FILE)

    parse_page(args.page_number)
