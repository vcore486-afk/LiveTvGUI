import requests
from bs4 import BeautifulSoup
import os
import argparse

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
def parse_page(page_number):
    # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å —É—á–µ—Ç–æ–º –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    URL = f"https://livetv869.me/allupcomingsports/{page_number}/"

    # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    homePath = os.path.expanduser("~")
    output_dir = os.path.join(homePath, ".livetv")
    os.makedirs(output_dir, exist_ok=True)

    OUTPUT_FILE = os.path.join(output_dir, "matchday_events.txt")  # –í—Å–µ–≥–¥–∞ –æ–¥–Ω–æ –∏–º—è —Ñ–∞–π–ª–∞

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
    try:
        response = requests.get(URL, headers={"User-Agent": "Mozilla/5.0"})
        response.raise_for_status()
        html_content = response.text
    except requests.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return

    # 2. –ü–∞—Ä—Å–∏–º HTML
    soup = BeautifulSoup(html_content, "lxml")

    # 3. –ù–∞—Ö–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ "–ì–ª–∞–≤–Ω—ã–µ –º–∞—Ç—á–∏ –¥–Ω—è"
    header = soup.find("b", text="–ì–ª–∞–≤–Ω—ã–µ –º–∞—Ç—á–∏ –¥–Ω—è")
    if not header:
        print("‚ùå –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return

    container = header.find_parent("table")
    matches_found = 0

    # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª
    with open(OUTPUT_FILE, "w", encoding="utf-8") as out:  # –ó–¥–µ—Å—å –Ω–µ –º–µ–Ω—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        out.write(f"üî• –ì–ª–∞–≤–Ω—ã–µ –º–∞—Ç—á–∏ –¥–Ω—è (–ø–µ—Ä–≤—ã–µ 10) - –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number}:\n\n")

        for a in container.find_all_next("a", class_="live"):
            match = a.get_text(strip=True)
            raw_href = a["href"].lstrip("/")  # —É–±–∏—Ä–∞–µ–º –≤–µ–¥—É—â–∏–π —Å–ª—ç—à

            # –£–±–∏—Ä–∞–µ–º "allupcomingsports/" –µ—Å–ª–∏ –µ—Å—Ç—å
            if raw_href.startswith("allupcomingsports/"):
                raw_href = raw_href[len("allupcomingsports/"):]

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Å—ã–ª–∫—É –≤–∏–¥–∞: https://livetv869.me/eventinfo/...
            link = f"https://livetv869.me/{raw_href}"

            evdesc = a.find_next("span", class_="evdesc")
            if not evdesc:
                continue

            lines = list(evdesc.stripped_strings)
            date_time = lines[0]
            league = lines[-1].strip("()")

            out.write(f"{league}\t{match}\n")
            out.write(f"{date_time}\n")
            out.write(f"({league})\n")
            out.write(f"{link}\n\n")

            matches_found += 1
            if matches_found == 10:
                break

    if matches_found == 0:
        print("‚ùå –ú–∞—Ç—á–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    else:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –º–∞—Ç—á–µ–π: {matches_found}")
        print(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {OUTPUT_FILE}")

# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ü–∞—Ä—Å–∏–Ω–≥ –º–∞—Ç—á–µ–π —Å —Å–∞–π—Ç–∞ livetv869.me")
    parser.add_argument("page_number", type=int, help="–ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
    args = parser.parse_args()

    # –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    parse_page(args.page_number)
